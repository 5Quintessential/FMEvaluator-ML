{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    " - Converts text into numeric vector of distinct words\n",
    " - Related documents from a product line become a single corpus\n",
    "   [If only one document is available then that would be the corpus as well]\n",
    " - When converting text to numeric data, create a dictionary of the numeric value and the corresponding\n",
    "   word and word-synonyms (using enchant)\n",
    " - Use the corpus for computing TF-IDF scores\n",
    " - The top 5 words with the highest score will likely be in the main subject of the corpus\n",
    "   So manually select the appropriate word that identifies the topic of the corpus from the top words\n",
    " - Starting from the topic word phrase, find phrases that are surrounding that phrase (word window classification) \n",
    "   [extract all the valid, non-repeating phrases] get all valid phrases that can be candidate features.\n",
    " - Manually mark all features and non-features based on the experts FM.\n",
    " - Then apply Logistic regression processing and get the best fit model with the least misclassification rate.\n",
    " - Compute the relations score table based on the heuristics mentioned in the paper\n",
    " - Any feature with a relation score greater than 0.9 if connected with another feature with a relation score \n",
    "   greater than 0.9 is considered a Mandatory relation. Otherwise it is considered as an Optional relation.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import library\n",
    "import numpy as np\n",
    "import re\n",
    "import enchant\n",
    "import itertools\n",
    "import math\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk import pos_tag\n",
    "from itertools import chain\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StatisticalFMEvaluator:\n",
    "    \n",
    "    corpus = [] # stores the document array in numeric representation\n",
    "    docs = [] # string array that stores plain text of each document\n",
    "    corpusname = '' # for creating unique output file names for different case studies\n",
    "    corpus_dict = {} # dictionary of all distinct word in the corpus\n",
    "    wordCounter = 0 # counter for tracking the total number of distinct words in the corpus\n",
    "    doc_bow_arr = [] # array with document bag of words format\n",
    "    doc_tags_arr = [] # list to store lists of post tags with the corresponding word for every document\n",
    "    features_lst = [] # list of all possible feature terms in the corpus\n",
    "    tags_lst = [] # list of all tags\n",
    "    \n",
    "    eng_dict = enchant.Dict(\"en_US\")\n",
    "    \n",
    "    def __init__(self, doc_arr, name):\n",
    "        self.docs = doc_arr\n",
    "        self.corpusname = name\n",
    "        \n",
    "                \n",
    "    def updateDictionary(self, wlist):\n",
    "        corpus_counter = []\n",
    "        # Search in dictionary\n",
    "        for w in wlist:\n",
    "            if w and not w.isspace():\n",
    "                if w not in self.corpus_dict:\n",
    "                    # search in dict values\n",
    "                    foundKey = self.getKeyByValue(w)\n",
    "                    if foundKey == '-1':\n",
    "                        # not in the dictionary, then it is a new word\n",
    "                        # add it\n",
    "                        self.wordCounter = self.wordCounter + 1\n",
    "                        self.corpus_dict[w] = (self.wordCounter, [w]) \n",
    "                        corpus_counter.append(self.wordCounter)\n",
    "                    else:\n",
    "                        old_value = self.corpus_dict[foundKey] # this is a tuple\n",
    "                        tmpLst = set(old_value[1])\n",
    "                        tmpLst.add(w)\n",
    "                        new_value = old_value[0], list(tmpLst)\n",
    "                        self.corpus_dict[foundKey] = new_value \n",
    "                        corpus_counter.append(old_value[0])                     \n",
    "\n",
    "                else:\n",
    "                    corpus_counter.append(self.corpus_dict[w][0])\n",
    "                        \n",
    "        return corpus_counter\n",
    "    \n",
    "    def getFeatureTermsList(self, topic, ngram):\n",
    "        selectedTags = ['NN','NNS','NNP','NNPS','VB','VBD','VBG','VBN','VBP','VBZ']\n",
    "        phraseTokenCnt = 0\n",
    "        phrases = []\n",
    "        tags = []\n",
    "        for tagTuples in self.doc_tags_arr:  \n",
    "            if tagTuples != \"#d\":\n",
    "                for (w,t) in tagTuples:\n",
    "                    if len(w)>3 and t in selectedTags and phraseTokenCnt <= 1 + 2*ngram:\n",
    "                        phrases.append(w)\n",
    "                        tags.append(t)\n",
    "                        phraseTokenCnt = phraseTokenCnt + 1\n",
    "                    else:\n",
    "                        if len(phrases) > 0:\n",
    "                            self.features_lst.append(\" \".join(phrases))\n",
    "                            self.tags_lst.append(\",\".join(tags))\n",
    "                            phrases = [] # clear the list\n",
    "                            tags = [] # clean the tags\n",
    "                            phraseTokenCnt = 0 # reset counter\n",
    "        featureTerms = [ph for ph in self.features_lst if topic in ph]\n",
    "        # collect 10 chained terms connected to each of the phrases in the above list\n",
    "        indexes = [self.features_lst.index(p) for p in featureTerms]\n",
    "        return self.getRanges(indexes)\n",
    "    \n",
    "    def getRanges(self, idx):\n",
    "        allIdx = []\n",
    "        for ind in idx:\n",
    "            allIdx.append(list(range(ind-5, ind+5)))\n",
    "        return list(itertools.chain.from_iterable(allIdx))\n",
    "    \n",
    "    def getDocTagsArray(self):\n",
    "        return self.doc_tags_arr\n",
    "    \n",
    "    def getAllFeaturesLst(self):\n",
    "        return self.features_lst \n",
    "    \n",
    "    def getAllFeaturesTagsLst(self):\n",
    "        return self.tags_lst                         \n",
    "            \n",
    "    def getAlphaString(self, wordString):\n",
    "        return re.sub('[^A-Za-z]+', '', wordString, 0, re.I)  \n",
    "    \n",
    "    def getKeyByValue(self, wval):\n",
    "        opkey = \"-1\"\n",
    "        listOfItems = self.corpus_dict.items()\n",
    "        for k, (keyid, ws) in listOfItems:\n",
    "            if wval in ws or self.isSynonym(wval,ws):\n",
    "                return k\n",
    "        return opkey \n",
    "    \n",
    "    def isSynonym(self, wrd, wrdLst):\n",
    "        synonyms = wordnet.synsets(wrd)\n",
    "        lemmas = set(chain.from_iterable([word.lemma_names() for word in synonyms]))\n",
    "        for _w in wrdLst:\n",
    "            syn = wordnet.synsets(_w)\n",
    "            lem = set(chain.from_iterable([word.lemma_names() for word in syn]))\n",
    "            res = lemmas.intersection(lem)\n",
    "            return bool(res)\n",
    "        \n",
    "    def printCorpus(self):\n",
    "        for _arr in self.corpus:\n",
    "            print(_arr)  \n",
    "            \n",
    "    def printDictionary(self):\n",
    "        print(self.corpus_dict)\n",
    "        \n",
    "    def getString(self, _idx):\n",
    "        return [k for k,(_k,_v) in self.corpus_dict.items() if _k == _idx][0]\n",
    "    \n",
    "    def getCorpusDictionary(self):\n",
    "        return self.corpus_dict\n",
    "    \n",
    "    def getDocBowArr(self):\n",
    "        return self.doc_bow_arr\n",
    "        \n",
    "    def computeTfIdfScores(self):\n",
    "        docArr = []\n",
    "        score_dict = {}\n",
    "        \n",
    "        for sen_ls in self.corpus[0]: \n",
    "            if sen_ls != \"#d\":\n",
    "                docArr.append(' '.join(self.getString(x) for x in sen_ls))\n",
    "                self.doc_bow_arr = docArr  \n",
    "                vectorizer = TfidfVectorizer(stop_words = 'english')\n",
    "                vectors = vectorizer.fit_transform(docArr)\n",
    "                feature_names = vectorizer.get_feature_names()\n",
    "                scores = zip(feature_names, np.asarray(vectors.sum(axis=0)).ravel())\n",
    "                sorted_scores = sorted(scores, key=lambda x: x[1], reverse=True)\n",
    "                score_dict = {}\n",
    "                for item in sorted_scores:\n",
    "                    score_dict[item[0]] = item[1]\n",
    "        return score_dict\n",
    "        \n",
    "    def generateCorpus(self):        \n",
    "        for doc in self.docs:\n",
    "            print(\"####################### DOCUMENT ####################\")\n",
    "            # local variable for storing the POS tags tuple\n",
    "            tags_tuple_lst = []\n",
    "            # tokenize and get all distinct words with length > 2 and no special characters\n",
    "            # convert to lower case\n",
    "            wordArr = []\n",
    "            \n",
    "            doc_lwr = doc.lower()\n",
    "            # sentence tokenize\n",
    "            for sentence in sent_tokenize(doc_lwr):                \n",
    "                # word tokenize\n",
    "                wordsList = []\n",
    "                flag_final = False\n",
    "                clean_word = ''\n",
    "                \n",
    "                # compute pos tag\n",
    "                words = word_tokenize(sentence)\n",
    "                tags_tuple_lst.append(pos_tag(words))\n",
    "                \n",
    "                for word in words:\n",
    "                    if len(word) > 2:\n",
    "                        if not flag_final:\n",
    "                            word = clean_word+word                            \n",
    "\n",
    "                        clean_word = self.getAlphaString(word)\n",
    "                        if clean_word:\n",
    "                            # check if the spelling is correct, if not correct it\n",
    "                            if self.eng_dict.check(clean_word):\n",
    "                                wordsList.append(clean_word)\n",
    "                                flag_final = True\n",
    "                            else:\n",
    "                                word_sugg = self.eng_dict.suggest(clean_word)\n",
    "                                if word_sugg and len(word_sugg[0]) > len(clean_word):\n",
    "                                    wordsList.append(word_sugg[0])\n",
    "                                    flag_final = True\n",
    "                                else:\n",
    "                                    flag_final = False\n",
    "                            \n",
    "                if not flag_final and clean_word: # take the middle suggested word\n",
    "                    sugg = self.eng_dict.suggest(clean_word)\n",
    "                    if sugg:\n",
    "                        wordsList.append(sugg[len(sugg)//2]) # floor division\n",
    "                dictKey = self.updateDictionary(wordsList)   \n",
    "                if dictKey:\n",
    "                    wordArr.append(dictKey)\n",
    "            # update corpus\n",
    "            if len(self.corpus) > 0:\n",
    "                self.corpus.append(\"#d\") # end of document marker\n",
    "            self.corpus.append(wordArr)\n",
    "            if len(self.doc_tags_arr) > 0:\n",
    "                self.doc_tags_arr.append(\"#d\") # end of document marker\n",
    "            self.doc_tags_arr.append(list(itertools.chain.from_iterable(tags_tuple_lst)))     \n",
    "            \n",
    "    def getSimilarMeaningListOfWords(self, words):\n",
    "        # check if the words are in the dictionary\n",
    "        wordSeq = []\n",
    "        for w in words:\n",
    "            if w not in self.corpus_dict:\n",
    "                # search in dict values\n",
    "                foundKey = self.getKeyByValue(w)\n",
    "            else:\n",
    "                foundKey = w\n",
    "                \n",
    "            if foundKey != '-1':\n",
    "                listOfItems = self.corpus_dict.items()\n",
    "                wordSeq.append([k for k, (keyid, ws) in listOfItems if foundKey==k])\n",
    "                \n",
    "        # just pick the tags of the words from the tags array\n",
    "        resultTags = []\n",
    "        for _wrd in wordSeq:\n",
    "            aTgs = set([_t for (_w,_t) in self.doc_tags_arr[0] if _w == _wrd])\n",
    "            # select the one which starts with 'N' or 'V'\n",
    "            selectedTags = [i for i in list(aTgs) if i.startswith('N') or i.startswith('V')]\n",
    "            if selectedTags:\n",
    "                resultTags.append(selectedTags[0])\n",
    "        return resultTags\n",
    "    \n",
    "    def getFeatureIndex(self, phrase):\n",
    "        ph_wrds = phrase.split(' ')\n",
    "        intRes = [len(list(set(ph_wrds).intersection(set(feature.split(' '))))) for idx, feature in enumerate(self.features_lst)]\n",
    "        max_value = max(intRes)\n",
    "        scoreIdx = intRes.index(max_value)\n",
    "        if scoreIdx >= 0:\n",
    "            return scoreIdx # return the first occurence\n",
    "        return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RegressionAnalysis:\n",
    "    featureSet = [] # list of possible features terms\n",
    "    allProbFeatures = [] # List of all possible feature terms\n",
    "    allFeaturesTags = [] # List of all possible feature terms POS tags\n",
    "    regression_data = pd.DataFrame() # Dataframe to store the regression analysis input data\n",
    "    data_columns = [] # List of columns used in the regression data\n",
    "    predictor_coefficients = {} # dictionary that stores all then significant variable coefficients\n",
    "    \n",
    "    def __init__(self, features, allFeatures, allFeatureTags):\n",
    "        self.featureSet = features\n",
    "        self.allProbFeatures = allFeatures\n",
    "        self.allFeaturesTags = allFeatureTags\n",
    "        \n",
    "    def computeRegressionData(self):\n",
    "        # get all distinct tag names for column data\n",
    "        distinct_tags = []\n",
    "        for _atag in self.allFeaturesTags:\n",
    "            distinct_tags.append(_atag.split(\",\"))\n",
    "        column_names = set(itertools.chain.from_iterable(distinct_tags))\n",
    "        column_names = [c.replace('PRP$','PRPD').replace('WP$','WPD').replace('$', 'POS') for c in column_names]\n",
    "        _columns = ['Feature'] + list(set(column_names))\n",
    "        self.data_columns = _columns\n",
    "        print(_columns)\n",
    "\n",
    "        # mark feature and non-feature\n",
    "        statdf = pd.DataFrame(columns=_columns)\n",
    "        statIdx = 0\n",
    "        for _combo in self.allFeaturesTags:\n",
    "            statdf.loc[statIdx] = [0] + [0]*(len(_columns)-1)\n",
    "            for _tag in _combo.split(\",\"):\n",
    "                if _tag == '$':\n",
    "                    _tag = 'POS'\n",
    "                if _tag == 'PRP$':\n",
    "                    _tag = 'PRPD'\n",
    "                if _tag == 'WP$':\n",
    "                    _tag = 'WPD'               \n",
    "                \n",
    "                statdf.loc[statIdx][_tag] = statdf.loc[statIdx][_tag] + 1\n",
    "            if statIdx in self.featureSet:\n",
    "                statdf.loc[statIdx]['Feature'] = 1\n",
    "            \n",
    "            statIdx = statIdx + 1        \n",
    "        # print(statdf)\n",
    "        self.regression_data = statdf\n",
    "        return statdf\n",
    "    \n",
    "    def modelGeneration_Coefficients(self, fileName):\n",
    "        reg_data = pd.read_csv(fileName)\n",
    "        # print(reg_data.head())\n",
    "        data_input_train, data_input_test, data_target_train, data_target_test = train_test_split(reg_data.drop(['Feature'],axis=1),reg_data['Feature'], test_size=0.2,random_state=3)\n",
    "        reg = LogisticRegression(solver='lbfgs')\n",
    "        pd.options.display.max_rows = None\n",
    "        reg.fit(data_input_train, data_target_train)\n",
    "        coeff = reg.coef_\n",
    "        \n",
    "        model_coefficients = {}\n",
    "        i = 1\n",
    "                           \n",
    "        for _coeff in coeff[0]:\n",
    "            model_coefficients[self.data_columns[i]] = _coeff\n",
    "            i = i + 1\n",
    "        self.predictor_coefficients = model_coefficients\n",
    "        # print(data_input_test.head())\n",
    "        print(self.predictor_coefficients)\n",
    "     \n",
    "    def PredictFeatureProbability(self, pos_tag_seq):\n",
    "        predicted_sum = 0\n",
    "        allowedTags = [_t for _t in self.data_columns if _t != 'Feature']\n",
    "        for _tag in pos_tag_seq:\n",
    "            if _tag in allowedTags:\n",
    "                predicted_sum = predicted_sum + self.predictor_coefficients[_tag]\n",
    "            else:\n",
    "                predicted_sum = predicted_sum + 0\n",
    "        predicted_prob = self.inv_logit(predicted_sum)\n",
    "        return predicted_prob                              \n",
    "     \n",
    "    def inv_logit(self, p):\n",
    "        return 1 / (1 + math.exp(-p))  \n",
    "    \n",
    "    # common function\n",
    "    def isSeqIn(self, a, b):\n",
    "        subseq = list((map(lambda x: b[x:x + len(a)] == a, range(len(b) - len(a) + 1))))\n",
    "        if True in subseq:\n",
    "            return subseq.index(True)\n",
    "        else:\n",
    "            # scan the words and check if the word contains the feature word\n",
    "            for w in a:\n",
    "                plurals = [i for i,p in enumerate(b) if w in p]\n",
    "                if plurals:\n",
    "                    return plurals[0] # return the first or default                \n",
    "        return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RelationsPredictor:\n",
    "    featureSet = [] # list of possible features terms\n",
    "    rootFeatureWord = '' # The word that MUST be in the root feature term\n",
    "    resultDistanceScores = [] # List storing the distance score values\n",
    "    \n",
    "    def __init__(self, topic, allFeatures):\n",
    "        self.rootFeatureWord = topic\n",
    "        self.featureSet = allFeatures\n",
    "        \n",
    "    def distance(self, lst, K):       \n",
    "        lst = np.asarray(lst) \n",
    "        diffArr = np.abs(lst - K)\n",
    "        idx = (np.abs(lst - K)).argmin() \n",
    "        return diffArr[idx] \n",
    "\n",
    "    def computeDistanceScores(self):\n",
    "        # find index of all phrases with the root feature word\n",
    "        idxs = [i for i, s in enumerate(self.featureSet) if self.rootFeatureWord in s]\n",
    "        \n",
    "        for ind, val in enumerate(self.featureSet):            \n",
    "            # Find the closest root feature term\n",
    "            self.resultDistanceScores.append(self.distance(idxs, ind))\n",
    "            \n",
    "        # Distance scores\n",
    "        dist_inp_minmax = []\n",
    "        amin, amax = min(self.resultDistanceScores), max(self.resultDistanceScores)\n",
    "        for i, val in enumerate(self.resultDistanceScores):\n",
    "            dist_inp_minmax.append(1- ((val-amin) / (amax-amin)))\n",
    "        return dist_inp_minmax        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
